# -*- coding: utf-8 -*-
"""saclassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1If41n9aduTU3_-2dLXEUIRcXhAMnZdrP
"""

import tensorflow as tf

from settings import PathSettings


class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super().__init__()
        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential(
            [tf.keras.layers.Dense(ff_dim, activation="relu"), tf.keras.layers.Dense(embed_dim),]
        )
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

class TokenAndPositionEmbedding(tf.keras.layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super().__init__()
        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

vocab_size = 20000  # Only consider the top 20k words
maxlen = 200  # Only consider the first 200 words of each movie review

import os
import pickle

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report

data_file = os.path.join(PathSettings.DATA_FOLDER, "bag_data.pkl")
label_encoder = LabelEncoder()
model_name = "tf_transformer"

train = pd.read_csv(os.path.join(PathSettings.DATA_FOLDER, "train_dp.csv"))
valid = pd.read_csv(os.path.join(PathSettings.DATA_FOLDER, "val_dp.csv"))
test = pd.read_csv(os.path.join(PathSettings.DATA_FOLDER, "test_dp.csv"))

vectorizer = tf.keras.layers.TextVectorization(max_tokens=20000, output_sequence_length=200)

text_ds = tf.data.Dataset.from_tensor_slices(train["cus_comment"].tolist()).batch(128)
vectorizer.adapt(text_ds)

label_encoder.fit(train["label"])

model_save_path = os.path.join(PathSettings.MODEL_FOLDER, f"{model_name}.h5")
label_encoder.fit(train["label"])


x_train = vectorizer(np.array([[s] for s in train["cus_comment"].tolist()])).numpy()
x_val = vectorizer(np.array([[s] for s in valid["cus_comment"].tolist()])).numpy()

y_train = tf.keras.utils.to_categorical(label_encoder.transform(train["label"]))
y_val = tf.keras.utils.to_categorical(label_encoder.transform(valid["label"]))

x_test = vectorizer(np.array([[s] for s in test["cus_comment"].tolist()])).numpy()

y_test = tf.keras.utils.to_categorical(label_encoder.transform(test["label"]))

voc = vectorizer.get_vocabulary()
word_index = dict(zip(voc, range(len(voc))))
num_tokens = len(voc) + 2
embedding_dim = 100

x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)

x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)

if not os.path.exists(model_save_path):
    embed_dim = 32  # Embedding size for each token
    num_heads = 2  # Number of attention heads
    ff_dim = 32  # Hidden layer size in feed forward network inside transformer

    inputs = tf.keras.layers.Input(shape=(maxlen,))
    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)
    x = embedding_layer(inputs)
    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
    x = transformer_block(x)
    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    x = tf.keras.layers.Dropout(0.1)(x)
    x = tf.keras.layers.Dense(20, activation="relu")(x)
    x = tf.keras.layers.Dropout(0.1)(x)
    outputs = tf.keras.layers.Dense(3, activation="softmax")(x)

    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    my_callbacks = [
            tf.keras.callbacks.EarlyStopping(patience=3),
        ]

    model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
    history = model.fit(
        x_train, y_train, batch_size=32, epochs=20, validation_data=(x_val, y_val), callbacks=my_callbacks
    )
    # model.save("tf_transformer.h5")
else:
    model = tf.keras.models.load_model(model_save_path, custom_objects={'CustomLayer': TokenAndPositionEmbedding})

predicts = model.predict(x_test, batch_size=32)

predicted = np.argmax(predicts, axis=1)
predicted = tf.keras.utils.to_categorical(predicted)

acc = accuracy_score(y_test, predicted)
clf_report = classification_report(y_test, predicted)

with open(os.path.join(PathSettings.RESULT_FOLDER, f"{model_name}.txt"), "w", encoding="utf-8") as w:
    w.write(f"{acc}\n")
    w.write(clf_report)




